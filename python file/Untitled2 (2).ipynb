{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t8TZ8AWpaDr",
        "outputId": "52393553-c543-41be-8024-ac75aa173771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Input, LSTM, Dropout\n",
        "import math\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zsr-mKh97hTm"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orFBt3rlR2sS",
        "outputId": "8ac04bdf-cb22-459f-fd9f-3dcae62c0f09"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        " \n",
        "def clean_doc(article):\n",
        "            \n",
        "            #print('article before ',article)\n",
        "            tokens =  [word for word in article.split()]\n",
        "            table = str.maketrans('', '', punctuation)# remove punctuation from each token\n",
        "            tokens = [w.translate(table) for w in tokens]\n",
        "            tokens = [word for word in tokens if word.isalpha()]# remove remaining tokens that are not alphabetic\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            tokens = [w for w in tokens if not w in stop_words]# filter out stop words\n",
        "            tokens = [lemmatizer.lemmatize(word) for word in tokens if len(word) > 1 and not '@' in word]# filter out short tokens\n",
        "            #lemmatizer.lemmatize(word)\n",
        "           # print('article after',TreebankWordDetokenizer().detokenize(tokens))\n",
        "            return TreebankWordDetokenizer().detokenize(tokens)"
      ],
      "metadata": {
        "id": "gk5xTem_RrZz"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1v3PEJ9nglR",
        "outputId": "242edd56-bbd0-40bf-d660-7fef0d3e9818"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GAmyqgcbnLHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "GvADFc04pc5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/bbc\"\n",
        "import os;\n",
        "import sys;\n",
        "from google.colab import drive\n",
        "TEXT_DATA_DIR = '/content/drive/MyDrive/bbc'\n",
        "texts = [] # list of text samples\n",
        "labels = [] # list of label ids\n",
        "labels_name=[]\n",
        "index=0\n",
        "for name in os.listdir(TEXT_DATA_DIR):\n",
        "  if '.ipynb_checkpoints'==name:\n",
        "    continue\n",
        "  labels_name.append(name)\n",
        "  path = os.path.join(TEXT_DATA_DIR, name)\n",
        "  for fname in os.listdir(path):\n",
        "    fpath = os.path.join(path, fname)\n",
        "    args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
        "    with open(fpath, **args) as f:\n",
        "       t = f.read()\n",
        "       texts.append(clean_doc(t))\n",
        "    labels.append(index) \n",
        "  index+=1\n",
        "\n",
        "            #print(fname)\n",
        "                \n",
        "#print(texts)\n",
        "#print('nLabels = ', len(labels))\n",
        "#print('Classes are:\\n ')\n",
        "#for key in labels:\n",
        " #   print (key)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnAg6Ij-pjNE",
        "outputId": "7759bd6b-aca4-413f-c2b8-851781b9d08b"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business  politics  sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index # the dictionary \n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
      ],
      "metadata": {
        "id": "kXrFBu4-7J_h"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_matrix = to_categorical(np.asarray(labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "g0Yon4bC8Kb0"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain,xtest,ytrain,ytest= train_test_split(data, labels_matrix,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "mAs_0Q319naa"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "print('Indexing word vectors.')\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        " for line in f:\n",
        "   values = line.split(sep=' ')\n",
        "   word = values[0]\n",
        "   coefs = np.asarray(values[1:], dtype='float32')\n",
        "   embeddings_index[word] = coefs\n",
        "#for key, value in embeddings_index.items():\n",
        " #   print(key, ' : ', value)   \n",
        "#print('Found word vectors.' ,(embeddings_index.most_common()))\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPmmyd-TEUwK",
        "outputId": "d6938b90-9263-490d-db3a-30da61f807eb"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (7) Map the dataset dictionary of (words,IDs) to a matrix of the\n",
        "# embeddings of each word in the dictionary\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))#+1 to include the zeros vector for non-existing words\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print ('Shape of Embedding Matrix: ',embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSkLTwmbCNol",
        "outputId": "be4b6fb4-32bf-470b-82e9-1053f2f57503"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Embedding Matrix:  (21005, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,  EMBEDDING_DIM, \n",
        "weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,))\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FEi1dY_F0A5",
        "outputId": "d7b8c345-e9ed-4855-8a42-1936c3c330b3"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_22 (Embedding)    (None, 1000, 100)         2100500   \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 128)               117248    \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,218,135\n",
            "Trainable params: 2,218,135\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        " optimizer='rmsprop',\n",
        " metrics=['acc'])\n",
        "# happy learning!\n",
        "print(xtrain.shape,ytrain.shape,xtest.shape,ytest.shape)\n",
        "model.fit(xtrain, ytrain, validation_data=(xtest, ytest),\n",
        " epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjjhoKwpG50t",
        "outputId": "fd1927f9-7391-47d3-97aa-87825e4a52e7"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1150, 1000) (1150, 3) (288, 1000) (288, 3)\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 57s 6s/step - loss: 0.8062 - acc: 0.6661 - val_loss: 0.3350 - val_acc: 0.9201\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 53s 6s/step - loss: 0.2826 - acc: 0.9122 - val_loss: 0.2006 - val_acc: 0.9306\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 53s 6s/step - loss: 0.1725 - acc: 0.9461 - val_loss: 1.1329 - val_acc: 0.6944\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 53s 6s/step - loss: 0.3025 - acc: 0.9061 - val_loss: 0.1331 - val_acc: 0.9583\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 55s 6s/step - loss: 0.1093 - acc: 0.9678 - val_loss: 0.1036 - val_acc: 0.9688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa7a342d90>"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Acuracy on testing set:')\n",
        "model.evaluate(xtest,ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpXEzzaDQDT0",
        "outputId": "28e90280-e7eb-416f-fa73-020d4e9f1989"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acuracy on testing set:\n",
            "9/9 [==============================] - 4s 318ms/step - loss: 0.1036 - acc: 0.9688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10358762741088867, 0.96875]"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test):\n",
        "    cleanedTest=clean_doc(test)\n",
        "    #print(\"cleanedTestResult\",cleanedTestResult)\n",
        "    tokenizer.fit_on_texts([cleanedTest])\n",
        "    encoded_test= tokenizer.texts_to_sequences([cleanedTest])\n",
        "    #print(\"encoded_test\",encoded_test)\n",
        "    # pad sequencesmax_length\n",
        "    Xtest = pad_sequences(encoded_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    #print(\"xtest\",Xtest)\n",
        "    res=model.predict([Xtest])\n",
        "    label_id = np.argmax(res)\n",
        "    print(res)\n",
        "    for  ID in labels:\n",
        "        if label_id == ID:\n",
        "             print ('The category of article %s is %s' %(test,labels_name[ID]))\n",
        "             break"
      ],
      "metadata": {
        "id": "g_eDnpPtVtE3"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size =64 and applying lemmitization th accuracy =96 in training and 98 in testing\n",
        "#batch size =64 and without lemmitization th accuracy =93 in training and 96 in testing\n",
        "# batch size =128 and applying lemmitization th accuracy =95 in training and 96 in testing\n",
        "\n",
        "predict('i love football')\n",
        "predict('we expect higher revenue and wider profit because i want to expand my business')\n",
        "predict('he was a Democrat in america')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UarT776fXGP7",
        "outputId": "fce67afa-08fc-4259-a8e9-c248e4e52b41"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5388224  0.21855034 0.24262723]]\n",
            "The category of article i love football is sport\n",
            "[[0.05708252 0.65434134 0.28857616]]\n",
            "The category of article we expect higher revenue and wider profit because i want to expand my business is business\n",
            "[[0.33016875 0.29443458 0.37539667]]\n",
            "The category of article he was a Democrat in america is politics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q4bPeOB5EENI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "52TqfuOrpqsn"
      }
    }
  ]
}